<p>
<title>NHS Crawler - Sam Wright</title>
<h3>Web app for crawling and searching the NHS Choices website's conditions pages.</h3>
<p>
    <h4>The API</h4>
    <ul>
        <li>
            <a href="/api/crawler/start">/api/crawler/start</a> to start crawling the website. This will
            take days to complete. You can stop the crawler and/or restart the
            app, then start the crawler again and it will resume from where it
            stopped. See the log output or <a href="/api/crawler/status">/api/crawler/status</a> to monitor its
            progress. Newly-crawled pages are added to the index for searching every 10 seconds.
        </li>
        <li>
            <a href="/api/crawler/status">/api/crawler/status</a> to get the status of the crawler (as a json string
            of io.github.samwright.nhs.api.crawler.CrawlerStatus).
        </li>
        <li><a href="/api/crawler/stop">/api/crawler/stop</a> to stop the crawler if it was running</li>
        <li>
            <a href="/api/search/reindex">/api/search/reindex</a> to recreate the index from the crawled pages on disk. This
            can be called at any time, even while the crawler is running. This takes about a minute to complete, and
            its progress can be monitored at <a href="/api/search/status">/api/search/status</a>. You only need to do this if
            you copy json files yourself into the pages directory.
        </li>
        <li>
            <a href="/api/search/status">/api/search/status</a> to get the status of the indexer (as a json string of
            io.github.samwright.nhs.api.search.IndexingStatus).
        </li>
        <li>
            <a href="/api/search?q=mumps">/api/search?q=mumps</a> to search through the indexed pages for 'mumps' to get the
            best match, and return its NHS Choices URL.
        </li>
        <li>
            <a href="/api/search/multiple?q=mumps">/api/search/multiple?q=mumps</a> to search through the indexed pages for
            'mumps' to get multiple top matches (at most 10), and return them in order in a json string of a
            list of io.github.samwright.nhs.common.search.SearchResult.
        </li>
    </ul>
</p>
<p>
    <h4>Quickstart</h4>
    <ol>
        <li>Click <a href="/api/crawler/start">/api/crawler/start</a></li>
        <li>
            Click <a href="/api/search?q=mumps">/api/search?q=mumps</a> and try different queries. Newly-crawled pages will
            be searchable after at most a 10 second delay.
        </li>
        <li>
            Restart the app at any time and click <a href="/api/crawler/start">/api/crawler/start</a> to continue crawling
            from where it left off.
        </li>
    </ol>
</p>
<p>
    <h4>Options</h4>
    <ul>
        <li>
            maxPagesPerCrawl - maximum number of pages to crawl, useful for testing (default = -1, meaning
            no maximum).
        </li>
        <li>crawlerCount - number of crawlers to run concurrently (default = 8).</li>
        <li>
            dataDir - directory to store crawled pages, crawler metadata, and the lucene index. If not set,
            this will default to "${java.io.tmpdir}/io.github.samwright.nhs", unless you set...
        </li>
        <li>
            randomiseTmpDir - if true and dataDir hasn't been set, store data in
            "${java.io.tmpdir}/io.github.samwright.nhs${randomLong}". Useful for testing.
        </li>
        <li>
            deleteOnExit - if true, delete the data directory on exiting the app (default = false)
        </li>
        <li>
            indexIntervalSeconds - the time to wait before adding newly-crawled pages to the index.
        </li>
    </ul>
</p>

<h5>Author: Sam Wright</h5>
<h5>
    Uses:
    <a href="https://lucene.apache.org/">Apache Lucene</a>,
    <a href="https://projects.spring.io/spring-boot/">Spring Boot</a>,
    <a href="http://projects.spring.io/spring-cloud/">Spring Cloud (Eureka, Feign, Zuul)</a>,
    <a href="https://github.com/yasserg/api/crawler4j">Crawler4J</a>,
    <a href="https://jsoup.org/">jsoup</a>,
    <a href="https://www.docker.com/">Docker</a>.
</h5>
</html>
